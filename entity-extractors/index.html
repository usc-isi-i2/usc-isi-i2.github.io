<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DIG: Entity Extractors</title>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.2/css/bootstrap.min.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="css/index.css">
	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
		<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
		<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-62418606-1', 'auto');
		ga('send', 'pageview');
	</script>
</head>
<body>
	<nav class="navbar navbar-default navbar-inverse navbar-fixed-top">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#nav-links">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand scroll" href="http://usc-isi-i2.github.io/dig/"><img src="img/dig-brand.png" height="20" alt="DIG: Domain-specific Insight Graphs"></a>
			</div><!-- navbar-header -->
			<div class="collapse navbar-collapse" id="nav-links">
				<ul class="nav navbar-nav navbar-right">
					<li class="active"><a class="scroll" href="#home">Home</a></li>
					<li><a class="scroll" href="#slides">Slides</a></li>
					<li><a class="scroll" href="#about">About</a></li>
					<li><a class="scroll" href="#tutorial">Tutorial</a></li>
					<!--<li><a class="scroll" href="#pub">Publications</a></li>-->
					<li><a class="scroll" href="#people">People</a></li>
				</ul>        
			</div><!-- collapse navbar-collapse -->
		</div><!-- container -->
	</nav>

	<header id="home">
		<div class="container">
			<img class="dig-logo" src="img/dig.png" alt="Dig logo">
			<h1>Entity Extractors for Domain-Specific Insight Graphs</h1>
			<div class="affiliation">
				<div><a href="../"><img src="img/iig-logo.png" width="280"></a></div>
				<div><a href="http://www.isi.edu"><img src="img/isi-logo.png" width="250"></a></div>
				<div><a href="http://www.usc.edu"><img src="img/usc-shield-name-white.png"></a></div>
			</div>
		</div><!-- container -->
		<div class="scroll-btn-container hidden-xs">
			<a class="scroll-btn scroll" href="#slides"><i class="fa fa-angle-down"></i></a>
		</div>
	</header>
	
	<section id="slides">
		<div class="content">
			<div class="slides-wrapper">
				<iframe src="https://www.haikudeck.com/e/QGvSJbOOiZ/?isUrlHashEnabled=false&isPreviewEnabled=false&isHeaderVisible=false" width="640" height="541" frameborder="0" marginheight="0" marginwidth="0"></iframe><br/><span style="font-family: arial, sans-serif; font-size: 8pt;"><a title="Entity Extractors In One Hour Presentation" href="https://www.haikudeck.com/p/QGvSJbOOiZ/entity-extractors-in-one-hour?utm_campaign=embed&utm_source=webapp&utm_medium=text-link">Entity Extractors In One Hour</a> - Created with Haiku Deck, presentation software that inspires</span>
			</div>
		</div>
	</section>

	<section id="about" class="container">
		<h2>WHAT IS ENTITY EXTRACTOR?</h2>
		<div class="content">
			<p>tbd</p>
		</div>
	</section>

	<section id="tutorial" class="container">
		<h2>TUTORIAL: HOW TO USE THE TOOL</h2>
		
		<div class="content">
			<p>Here are the steps which need to take place when creating and managing an extraction</p>
			<h4>Enviroment</h4>
			<ol>
				<li>We will refer to your github root dir as $GITHUB. Your home directory is $HOME.</li>
				<li>Edit (at the bottom will work well) your ~/.profile or other sh init file (e.g. .bash_profile) to contain the following:</li>
					<p><code>export GITHUB=&lt;your github repository root folder&gt;<br/>
					export PATH="$PATH:$GITHUB/dig-mturk/generic/scripts</code></p>
				<li><code>. ~/.profile</code> (or other init file used above)</li>
				<li>Obtain from Andrew or Suba the AWS username, AWS Access Key ID, and AWS Secret Access Key</li>
				<li><code>pip install awscli</code></li>
				<li><code>aws configure</code></li>
				<ul>
					<li>AWS Access Key ID = key id obtained</li>
					<li>AWS Secret Access Key = secret key obtained</li>
					<li>Default region name = usc-west-2</li>
					<li>Default output format = json</li>
				</ul>
				<li>create $HOME/.aws/mturk_live.properties with contents</li>
					<p><code>
						# -------------------<br/>
						# ADVANCED PROPERTIES<br/>
						# -------------------<br/>
						# If you want to have your solution work against the Amazon Mechnical Turk Production site (http://www.mturk.com)<br/>
						# use the service_url defined below:<br/>
						service_url=https://mechanicalturk.amazonaws.com/?Service=AWSMechanicalTurkRequester<br/><br/>

						#list of comma separated retriable errors which will be retried by RetryFilter<br/>
						retriable_errors=Server.ServiceUnavailable<br/>
						retry_attempts=10<br/>
						retry_delay_millis=1000<br/>
						access_key=&lt;AWS ACCESS KEY ID&gt;<br/>
						secret_key=&lt;AWS SECRET ACCESS KEY&gt;
					</code></p>
				<li>create $HOME/.aws/mturk_sandbox.properties with contents</li>
					<p><code>
						# -------------------<br/>
						# ADVANCED PROPERTIES<br/>
						# -------------------<br/>
						# If you want to test your solution in the Amazon Mechanical Turk Developers Sandbox (http://sandbox.mturk.com)<br/>
						# use the service_url defined below:<br/>
						service_url=https://mechanicalturk.sandbox.amazonaws.com/?Service=AWSMechanicalTurkRequester<br/><br/>

						#list of comma separated retriable errors which will be retried by RetryFilter<br/>
						retriable_errors=Server.ServiceUnavailable<br/>
						retry_attempts=10<br/>
						retry_delay_millis=1000<br/>
						access_key=&lt;AWS ACCESS_KEY ID&gt;<br/>
						secret_key=&lt;AWS SECRET ACCESS KEY&gt;
					</code></p>
			</ol>
			<h4>Requirements</h4>
			<ol>
				<li>Install python 2.7, java 1.7, maven 3 using system installers (apt-get, yum, Mac installer, etc.)
				<li>Install Web-Karma, dig-mturk in $GITHUB</li>
				<li><code>pip install requests</code></li>
				<li><code>pip install boto</code></li>
				<li><code>pip install nltk</code></li>
				<li><code>cd $GITHUB/dig-mturk; git checkout refactor; cd mturk; mvn clean install</code></li>
				<li><code>cd $GITHUB/Web-Karma; git checkout development; mvn clean install</code></li>
			</ol>
			<h4>SETUP</h4>
			<ol>
				<li><code>cd $GITHUB/dig-mturk</code></li>
				<li>Determine the name of the extraction. Examples are 'eyehair' and 'ethnic'</li>
				<li>For convenience, export <code>EXTRACTION=&lt;extraction&gt;</code></li>
				<li><code>newExtraction.sh $EXTRACTION</code> This will create $GITHUB/dig-mturk/extractions/$EXTRACTION and copy/adapt a few files there</li>
				<ul>
					<li>create.cfg</li>
					<li>hitdata.pyfmt</li>
					<li>fetchSentences.sh, query.json</li>
					<li>head.html, instructions.html, tail.html</li>
					<li>$EXTRACTION-context.json, $EXTRACTION-model.png, $EXTRACTION-model.ttl, $EXTRACTION-report.md</li>
					<li>qualification/*</li>
					<li>karma/</li>
				</ul>
				<li>Edit these configuration files. You will want a small set of categories and associated labels. The category name itself should be a simple lower case identifier without punctuation. The label can be rendered in English. For example, in 'eyehair' we might have categories 'eye' and label ('Eye Color') and 'hair' (label 'Hair Type').</li>
				<ul>
					<li>create.cfg: Edit as desired number of hits, number of sentences, elastic search path is very important to verify</li>
					<li>hitdata.pyfmt: Edit the task description, title, and categories. You should leave the scratch_category as it is. You may wish to edit the keywords. You should leave {instructions} and {sentences} as they are. To start, qualification should be null, which is expressed by having all the qualification parameters be empty strings. Do not just delete the qualification block.</li>
					<li>fetchSentences.sh: edit the ES index where data will be obtained, number of ES hits desired, etc.</li>
					<li>query.json: edit the query (see <a url://elasticsearch.co/>Elastic Seach</a>)</li>
					<li>head.html: no change</li>
					<li>instructions.html: change the sample content and the categories to conform to your categories and likely data. There are five necessary modifications in instructions.html. Each is introduced by a comment suggesting what needs to be changed.</li>
					<li>tail.html: change the tail.html if you consider the boilerplate text confusing in context</li>
					<li>you can inspect the look of the instructions by performing: <code>cat head.html instructions.html tail.html > page.html; open page.html</code></li>
					<li>qualification/*</li>
					<li>karma/</li>
				</ul>
				<li>Edit the karma/preloaded-ontologies/mturk-ontology.ttl: add/edit relation(s) corresponding to the categories you are defining</li>
				<li>Edit the karma/python/mturk.py: for each relation there should be a mapping to a category; they may be the same string if legal TTL identifier, or you can make the category name a pretty name (i.e., with spaces, punctuation)</li>
				<li><code>fetchSentences.sh</code></li>
			</ol>
			<h4>SandBox Dry Run</h4>
			<ol>
				<li>Install python 2.7, java 1.7, maven 3 using system installers (apt-get, yum, Mac installer, etc.)
				<li>Install Web-Karma, dig-mturk in $GITHUB</li>
				<li><code>pip install requests</code></li>
				<li><code>pip install boto</code></li>
				<li><code>pip install nltk</code></li>
				<li><code>cd $GITHUB/dig-mturk; git checkout refactor; cd mturk; mvn clean install</code></li>
				<li><code>cd $GITHUB/Web-Karma; git checkout development; mvn clean install</code></li>
			</ol>
			<h4>Generate Qualification Task</h4>
			<p>To generate a qualification task, you need the following:</p>
			<ol>
				<li>name of your qualification. Let's call it $NAME 1. categories for this domain</li>
				<li>example sentences</li>
				<li>the answers from those sentences</li>
				<li>explanations of the answers (used to provide feedback during qualification testing)</li>
				<li>go to directory qualification/</li>
				<li>Make a copy of qual_boilerplate.json as qual_$NAME.json</li>
				<li>Edit this to reflect categories, sentences, answers, etc. to define your qualification domain. You will see the explanation.wrong field which suggests to use another script (explanationGenerator.py) to create the value. You will need to edit this file/create configuration parameters (TBD) for each question to do so. The results can be copied into the explanation.wrong field when complete.</li>
				<li>When the qual_$NAME.json file is complete it must be uploaded to S3</li>
			</ol>
			<h4>Tailor Qualifications</h4>
				<p>With the sandbox examples in place, you can generate the question texts, categories, answer images/answer texts which can be used in the qualification task. You can edit these resources into $EXTRACTION/qualification/qual_$EXTRACTION.json. URL resources will reside in <a>https://s3-us-west-2.amazonaws.com/aisoftwareresearch/extractions/$EXTRACTION/qualification/config/</a> or <a>https://s3-us-west-2.amazonaws.com/aisoftwareresearch/extractions/$EXTRACTION/qualification/image/</a>.<p>
			<h4>Live Scale Run</h4>
			<ol>
				<li>for sandbox, choose a trial name (e.g., trial01, live01)</li>
				<li><code>mkdir $GITHUB/dig-mturk/extractions/$EXTRACTION/$TRIAL</code></li>
				<li><code>./createHits.sh $EXTRACTION $TRIAL</code></li>
				<li><code>deployHits.sh -live $EXTRACTION $TRIAL</code></li>
				<li>When turkers have completed 3 sandbox tests on all hits</li>
				<li><code>hitResults.sh -sandbox $EXTRACTION $TRIAL</code></li>
				<li><code>consolidateResults.sh $EXTRACTION $TRIAL</code># Note: no -live</li>
				<li><code>fetchConsolidated.sh $EXTRACTION $TRIAL</code></li>
				<li>eventually, karma processing will be done via script:</li>
				<ul>
					<li><code>modelConsolidated.sh $EXTRACTION $TRIAL</code></li>
					<li>for now, need to manually run karma on the trial data and the extraction model (TBD)</li>
				</ul>
				<li><code>adjudicate.sh $EXTRACTION $TRIAL</code></li>
				<li>adjudicated results are in $GITHUB/dig-mturk/extractions/$EXTRACTION/$TRIAL/adjudicated_$EXTRACTION_$TRIAL.json and can be passed to David Stallard/Daniel Marcu for CRF++ learning</li>
				<li>TBD: train CRF++</li>
				<li>TBD: apply trained CRF++ extractor to data</li>
				<li>TBD: karma model for CRF++ extracted data</li>
				<li>TBD: integrate karma mode for CRF++</li>
			</ol>
		</div>
	</section>
	<!--
	<section id="pub" class="container">
		<h2>PUBLICATIONS</h2>
		<ul class="content list-unstyled">
			<li>
				<div class="title"><a href="http://www.isi.edu/integration/papers/knoblock15-spie.pdf">A Scalable Architecture for Extracting, Aligning, Linking, and Visualizing Multi-Int Data</a></div>
				<div class="author">Knoblock, C. A. and Szekely, P.</div>
				<div class="location">In Proceedings of the Conference on the Next Generation Analyst III, 2015. SPIE, 9499</div>
			</li>
			<li>
				<div class="title"><a href="http://www.forbes.com/sites/thomasbrewster/2015/04/17/darpa-nasa-and-partners-show-off-memex">Watch Out Google, DARPA Just Open Sourced All This Swish 'Dark Web' Search Tech</a></div>
				<div class="author">Forbes</div>
			</li>
                       <li>
				<div class="title"><a href="http://www.wired.co.uk/news/archive/2015-05/19/human-trafficking-data-hack">The escort database that combats human trafficking</a></div>
				<div class="author">wired.co.uk</div>
			</li>
		</ul>
	</section>
	-->
	<section id="people" class="container">
		<h2>PEOPLE</h2>
		<ul class="content list-unstyled">
			<li>
				<a style="background-image:url('img/people/Pedro-Szekely.jpg')" href="../szekely"></a>
				<div class="description">
					<div class="name">Pedro Szekely</div>
					<div class="title">Project Leader & Research Associate Professor</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Craig-Knoblock.jpg')" href="../knoblock"></a>
				<div class="description">
					<div class="name">Craig Knoblock</div>
					<div class="title">Director & Research Professor</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Kevin-Knight.jpg')" href="http://www.isi.edu/~knight"></a>
				<div class="description">
					<div class="name">Kevin Knight</div>
					<div class="title">Director & Professor</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Daniel-Marcu.jpg')" href="http://www.isi.edu/~marcu"></a>
				<div class="description">
					<div class="name">Daniel Marcu</div>
					<div class="title">Director & Research Associate Professor</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Andrew-Philpot.jpg')" href="http://www.isi.edu/~philpot/"></a>
				<div class="description">
					<div class="name">Andrew G. Philpot</div>
					<div class="title">Computer Scientist</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Subessware-S-K.jpg')" href="https://www.linkedin.com/in/subessware"></a>
				<div class="description">
					<div class="name">Subessware S K</div>
					<div class="title">Student</div>
				</div>
			</li>
			<li>
				<a style="background-image:url('img/people/Lidia-Ferreira.jpg')" href="http://homepages.dcc.ufmg.br/~lidiaferreira/indexen.html"></a>
				<div class="description">
					<div class="name">Lidia Ferreira <sup>1</sup></div>
					<div class="title">Student</div>
					<!--test-->
				</div>
			</li>
		</ul>
	</section>

	<section class="container">
		<h2>ACKNOWLEDGMENT</h2>
		<p class="content">This research is supported by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under contract number FA8750-14-C-0240.</p>
		<p class="content"><sup>1</sup> The student Lidia Ferreira thank to Science Without Borders Program, CAPES Scholarship - Proc. NÂº 88888.030514/2013-00.</p>
	</section>

	<footer>
		<div class="container">&copy; 2015 The University of Southern California</div>
	</footer>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.2/js/bootstrap.min.js"></script>
	<script src="js/index.js"></script>
</body>
</html>
